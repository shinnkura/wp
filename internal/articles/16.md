{
"Title": "ディープラーニング入門：ニューラルネットワークと Pytorch を活用した学習の仕組み",
"Image": "pytorch.png",
"Permalink": "deeplearning-intro",
"Tag": [
"機械学習",
"ディープラーニング",
"Pytorch"
],
"Category": [
"AI"
],
"post_id": 3773
}

---

## ディープラーニングとは？

ディープラーニング（DL）とは、ニューラルネットワークの層を深くしたものです。ニューラルネットワークとは、人間の脳の神経回路（ニューロン）の仕組みをモデルにした機械学習アルゴリズムです。このニューラルネットワークを多層にし、データのパターンを学習することで、高度な予測や分類を可能にします。

## モデルとディープラーニング

DL の中には、モデルという概念があります。モデルとは、プログラミングにおける関数のようなもので、入力を受け取り、出力を返すものです。
最初、モデルはデタラメな出力を返します。そして、このモデルを学習させていくことで、「**未知のデータ**に対する予測精度(能力)が高いモデル」を作っていく。これが DL です。

## モデルの種類

モデルには様々な種類があります。中でも有名なのは以下の３つです。

  - MLP：他の DL のベースとなっているモデル
  - CNN：画像分野に特化したモデル
  - RNN：自然言語処理に特化したモデル

この記事では、最も基本である MLP について解説します。

## 全結合層（Fully Connected Layer）

MLP は 3 つの層で構成されます。

1. **入力層**: データの入り口（例: 「部屋の広さ」「築年数」「駅からの距離」）
2. **中間層（隠れ層）**: データを加工する層（hidden layer）
3. **出力層**: 予測結果（例: 「家賃」）

MLP では、入力層から出力層まで、ノードを重ねて計算を行います。
その際に、以下の 3 つの要素を用いて計算を行います。

   - **バイアス項**: ノードの入力に加算される定数
   - **パラメータ（重み）**: ノード間のつながりの強さを表す値
   - **エッジ**: ノード同士をつなぐ線

各層のバイアス項とパラメータを掛けて計算を行ったノードたちを重ねていくことで、出力層で予測結果を出すことができます。

## 教師あり学習

MLP は基本的に「教師あり学習」として分類されます。「教師あり学習」とは、入力データとそれに対する正解データを用いて、モデルを学習させる手法です。

また、教師あり学習には以下の 2 種類があります。

- **回帰（連続値）**: 数値を予測する（家賃や株価などの数値予測）
- **分類（離散値）**: カテゴリーを分類する（画像認識（犬か猫か））

今回は、回帰の例を用いて解説します。

### Pytorch で全結合層を作成する方法

```python
import torch.nn as nn

nn.Linear(入力ノード数, 出力ノード数)
```

`入力層が3つ`、`中間層が2つ`、`出力層が1つ`の場合は、以下のように記述します。

```python
nn.Linear(3, 2)
nn.Linear(2, 1)
```

## DL でよく使用する計算

1. **線形変換**（ノードを重ねる際にウェイトをかける）
   - 名前にあるように、線（一次関数）に変換可能な計算で使用
   - 「入力層->中間層」「中間層->出力層」までのノードを重ねる際に使用

2. **非線形変換**（活性化関数を適用）
   - 線形変換では、線に変換可能な計算であるため、ニューラルネットワークの表現力が低い
   - 非線形変換を適用することで、ニューラルネットワークの表現力を高めることができる
   - 代表的な活性化関数には、ReLU 関数、シグモイド関数、ソフトマックス関数などがある
   - これらは、「中間層」で使用される

3. **目的関数（損失関数）を計算**（予測値と正解値の誤差を計算）
   - 予測値と正解値の誤差を計算することで、モデルの学習を行う
   - これらは、「出力層」で使用される

**順伝播**: 入力層 → 出力層のデータの流れ
**逆伝播**: 出力層 → 入力層のデータの流れ（誤差の修正）

## 線形変換

線形変換は、ノードを重ねる際に、ウェイト（パラメータ）を掛ける計算です。
主に、「入力層->中間層」「中間層->出力層」までのノードを重ねる際に使用されます。

**計算式:**

```math
y = Wx + b
```

- `y`: 出力値
- `W`: ウェイト（パラメータ）
- `x`: 入力値
- `b`: バイアス

## 非線形変換

これは、「中間層」で使用される計算です。
イメージとしては、線形変換では、線に変換可能な計算であるため、バラバラになっているノードを線に変換するのは難しいです。
そのため、バラバラになっているノードを多次元関数のような線に変換するために、非線形変換を使用します。
そうすることで、ノード内のデータを忠実に表現することができます。

また、非線形変換は、**活性化関数**を使用します。

### 代表的な活性化関数

  - **シグモイド関数**: 勾配消失問題があり、現在はほとんど使われない
  - **ReLU 関数（正規化線形関数）**: 現在最もよく使われる活性化関数
  - **ソフトマックス関数**: 出力層で使用される確率分布を求める関数

## 目的関数（損失関数・コスト関数）

予測値と目標値の誤差を測定する関数。

  - **回帰**: 平均二乗誤差（MSE: Mean Squared Error）
  - **分類**: 交差エントロピー誤差（Cross Entropy Error）

## 逆伝播とパラメータの更新

MLP において、一番重要といっても良いのが、**逆伝播**と**パラメータの更新**です。

これまでの計算で生じた、誤差を修正し、適切なパラーメターに更新してあげることで、ニューラルネットワークの精度を高めることができます。

この計算を行うためには、**最急降下法**と**誤差逆伝播法**を用います。

### 最急降下法（SGD: 確率的勾配降下法）

  - 勾配を求め、勾配方向にパラメータを更新
  - 勾配が 0 になる点で更新を停止
  - ただ、多次元関数のように、複数の傾きが０になる点が存在する場合、最も小さい点である「大域的最適解」ではなく、「居所最適解」に陥る可能性がある
  - また、勾配が大きすぎて、パラメータの更新がうまくいかず、勾配が 0 になってしまう「勾配爆発」という危険性もある
  - そのために、勾配を小さくする工夫が必要で、「学習係数（0.01）」を勾配にかけてあげることで、勾配を小さくすることができる

## Pytorch によるニューラルネットワークの実装

### 全結合層の作成

```python
import torch.nn as nn
fc = nn.Linear(3, 2)
fc.weight  # 重み
fc.bias    # バイアス
```

### 乱数のシードを固定

DL を行う際、デフォルトでは、ランダムな値を用いて計算を行います。
以下のように、乱数のシードを固定することで、同じ結果を得ることができます。

```python
import torch
torch.manual_seed(0)
```

### テンソルの作成

テンソルは、Pytorch で用いられるデータの型です。
Pytorch で計算を行う際は、テンソルを用いる必要があります。

```python
x = torch.tensor([1., 2., 3.])
print(x)  # tensor([1., 2., 3.])
print(type(x))  # torch.Tensor
print(x.dtype)  # torch.float32
```

### 線形変換の計算

```python
fc = nn.Linear(3, 2)
u = fc(x)
print(u)  # tensor([-0.1261, -0.1261], grad_fn=<AddBackward0>)
```

### 非線形変換（ReLU 関数）

```python
import torch.nn.functional as F
fc = nn.Linear(3, 2)
u = fc(x)
h = F.relu(u)
print(h)  # tensor([0.00000, 0.0526], grad_fn=<ReluBackward0>)
```

ReLU 関数は `max(0, x)` を計算し、0 以下の値を 0 に変換します。

### 目的関数の計算

```python
# 目標値
t = torch.tensor([1., 3.])
```

```python
# 予測値
y = torch.tensor([2., 4.])
```

```python
# 平均二乗誤差
mse = F.mse_loss(y, t)
print(mse)  # tensor(1.0000)
```

## まとめ

  - **ディープラーニングはニューラルネットワークの層を深くしたもの**
  - **順伝播と逆伝播を繰り返しながらパラメータを更新する**
  - **Pytorch を使って簡単にニューラルネットワークを構築できる**

Pytorch を活用しながら、ディープラーニングの理解を深めていきましょう！
